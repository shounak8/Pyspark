{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b282e3",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "PySpark is an interface for Apache Spark in Python. It not only allows you to write Spark applications using Python APIs, but also provides the PySpark shell for interactively analyzing your data in a distributed environment.\n",
    "\n",
    "This notebook covers the Machine Learning libraries of PySpark.\n",
    "\n",
    "Process is simple.\n",
    "1. For every dataset, we create 2 columns: Features and Label (Label not applicable to Clustering & Recommendation Model)\n",
    "2. We form and train the model on training data\n",
    "3. We evaluate the model\n",
    "4. We test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fdd358",
   "metadata": {},
   "source": [
    "# ML Content:\n",
    "### 1. Regression\n",
    "### 2. Classification\n",
    "### 3. Clustering\n",
    "### 4. Recommendation System\n",
    "### 5. NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8998760b",
   "metadata": {},
   "source": [
    "# Common Actions:\n",
    "Below code will remain common for all ML processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "560060d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "404e6854",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/14 18:42:08 WARN Utils: Your hostname, shounak-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "21/10/14 18:42:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/shounak/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/10/14 18:42:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# initiating Pyspark session\n",
    "import findspark\n",
    "\n",
    "findspark.init(\"/home/shounak/spark-3.1.2-bin-hadoop3.2\")\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"demo\").getOrCreate()\n",
    "# We use pandas for\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston, load_iris\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Importing the methods that we will be using:\n",
    "from pyspark.sql.functions import udf, countDistinct\n",
    "\n",
    "# udf = User Defined Functions\n",
    "# countDistinct = Counts the number of distinct values in a column\n",
    "\n",
    "# Import Feature Processing Models\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, StandardScaler\n",
    "\n",
    "# VectorAssembler = Forms feature column,\n",
    "# StringIndexer = Converts String to Vector\n",
    "# StandardScaler = Normalises the numeric features\n",
    "\n",
    "# Import Evaluation Metrics\n",
    "from pyspark.ml import evaluation\n",
    "from pyspark.ml.evaluation import RegressionEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "# Import Machine Learning Models\n",
    "# Regression Model\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Classification Model\n",
    "from pyspark.ml.classification import (\n",
    "    GBTClassifier,\n",
    "    NaiveBayes,\n",
    "    RandomForestClassifier,\n",
    "    DecisionTreeClassifier,\n",
    "    MultilayerPerceptronClassifier,\n",
    ")\n",
    "\n",
    "# Clustering Model\n",
    "from pyspark.ml.clustering import KMeans\n",
    "import matplotlib.pyplot as plt  # Used for determining ideal cluster numbers using elbow-method\n",
    "\n",
    "# Recommendation Model\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "# For building NLP features and labels:\n",
    "from pyspark.ml.feature import (\n",
    "    RegexTokenizer,\n",
    "    StopWordsRemover,\n",
    "    HashingTF,\n",
    "    IDF,\n",
    "    StringIndexer,\n",
    ")\n",
    "\n",
    "# For building pipeline of methods:\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f62e42e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scaler function\n",
    "scaler = StandardScaler(inputCol=\"raw_features\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fa5c1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove null and duplicate values\n",
    "def rem_null_duplicates(dataframe):\n",
    "    dataframe = dataframe.dropna()\n",
    "    dataframe = dataframe.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "117f7ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def skdataset_to_sparkdataset(sklearn_dataset):\n",
    "    load_sklearn_dataset = sklearn_dataset()\n",
    "    pd_df = pd.DataFrame(\n",
    "        data=load_sklearn_dataset[\"data\"], columns=load_sklearn_dataset[\"feature_names\"]\n",
    "    )\n",
    "    pd_df[\"target\"] = load_sklearn_dataset[\"target\"]\n",
    "    #     pd_df.to_csv(\"temp_df.csv\", index=False)\n",
    "    #     spark_df = spark.read.csv(\"temp_df.csv\",inferSchema=True, header=True)\n",
    "    spark_df = spark.createDataFrame(pd_df)\n",
    "    return spark_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ee9adc",
   "metadata": {},
   "source": [
    "# 1. Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46a98842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+----+-----+-----+----+------+---+-----+-------+-----+-----+------+\n",
      "|   CRIM|  ZN|INDUS|CHAS|  NOX|   RM| AGE|   DIS|RAD|  TAX|PTRATIO|    B|LSTAT|target|\n",
      "+-------+----+-----+----+-----+-----+----+------+---+-----+-------+-----+-----+------+\n",
      "|0.00632|18.0| 2.31| 0.0|0.538|6.575|65.2|  4.09|1.0|296.0|   15.3|396.9| 4.98|  24.0|\n",
      "|0.02731| 0.0| 7.07| 0.0|0.469|6.421|78.9|4.9671|2.0|242.0|   17.8|396.9| 9.14|  21.6|\n",
      "+-------+----+-----+----+-----+-----+----+------+---+-----+-------+-----+-----+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Form dataset\n",
    "regression_df = skdataset_to_sparkdataset(load_boston)\n",
    "# Remove Null values & Duplicates\n",
    "rem_null_duplicates(regression_df)\n",
    "# Let us visualize dataset\n",
    "regression_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e953b6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use VectorAssembler to form features-column consisting of array of features out of the available features\n",
    "assembler_lin_reg = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"CRIM\",\n",
    "        \"ZN\",\n",
    "        \"INDUS\",\n",
    "        \"CHAS\",\n",
    "        \"NOX\",\n",
    "        \"RM\",\n",
    "        \"AGE\",\n",
    "        \"DIS\",\n",
    "        \"RAD\",\n",
    "        \"TAX\",\n",
    "        \"PTRATIO\",\n",
    "        \"B\",\n",
    "        \"LSTAT\",\n",
    "    ],\n",
    "    outputCol=\"raw_features\",\n",
    ")\n",
    "\n",
    "\n",
    "regression_df = assembler_lin_reg.transform(regression_df)\n",
    "\n",
    "# We will scale the features column\n",
    "regression_df = scaler.fit(regression_df).transform(regression_df)\n",
    "# Since we only want 2 columns \"features\" &\"target\" to train the model, we will not select other columns\n",
    "regression_df = regression_df.select(\"features\", \"target\")\n",
    "\n",
    "# Visualize the cleaned dataset\n",
    "regression_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "13efdd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train and test\n",
    "train_regression_df, test_regression_df = regression_df.randomSplit([0.8, 0.2])\n",
    "\n",
    "# form the model\n",
    "linear_regression = LinearRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"target\",\n",
    ")\n",
    "# train the model\n",
    "lin_reg_model = linear_regression.fit(train_regression_df)\n",
    "# Check the results on test data\n",
    "test_result = lin_reg_model.transform(test_regression_df)\n",
    "# Visualize the test data post transformation\n",
    "test_result.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ee0b4c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the evaluator\n",
    "evaluator_lin_reg = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"target\")\n",
    "# Check the RSME value\n",
    "evaluation_lin_reg = evaluator_lin_reg.evaluate(test_result)\n",
    "print(\"RSME for the Linear Regression model on test cases is \", evaluation_lin_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68986126",
   "metadata": {},
   "source": [
    "# 2. Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2aefc7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+-----------------+----------------+------+\n",
      "|sepal length (cm)|sepal width (cm)|petal length (cm)|petal width (cm)|target|\n",
      "+-----------------+----------------+-----------------+----------------+------+\n",
      "|              5.1|             3.5|              1.4|             0.2|     0|\n",
      "|              4.9|             3.0|              1.4|             0.2|     0|\n",
      "+-----------------+----------------+-----------------+----------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Form dataset\n",
    "classification_df = skdataset_to_sparkdataset(load_iris)\n",
    "# Remove Null values & Duplicates\n",
    "rem_null_duplicates(classification_df)\n",
    "# Let us visualize dataset\n",
    "classification_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7cde2b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use VectorAssembler to form features-column consisting of array of features out of the available features\n",
    "assembler_classification = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"sepal length (cm)\",\n",
    "        \"sepal width (cm)\",\n",
    "        \"petal length (cm)\",\n",
    "        \"petal width (cm)\",\n",
    "    ],\n",
    "    outputCol=\"raw_features\",\n",
    ")\n",
    "classification_df = assembler_classification.transform(classification_df)\n",
    "\n",
    "# We will scale the features column\n",
    "classification_df = scaler.fit(classification_df).transform(classification_df)\n",
    "\n",
    "# Since we only want 2 columns \"features\" &\"target\" to train the model, we will not select other columns\n",
    "classification_df = classification_df.select(\"features\", \"target\")\n",
    "\n",
    "# Visualize the cleaned dataset\n",
    "classification_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6bee702a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train and test\n",
    "train_classification_df, test_classification_df = classification_df.randomSplit(\n",
    "    [0.8, 0.2]\n",
    ")\n",
    "\n",
    "# GBTClassifier is limited to binary classification, so we will not use GBT in this case.\n",
    "\n",
    "# Form the models:\n",
    "# NaiveBayes model\n",
    "nb = NaiveBayes(labelCol=\"target\")\n",
    "# Random Forest model\n",
    "rf = RandomForestClassifier(labelCol=\"target\")\n",
    "# Decision Tree model\n",
    "dc = DecisionTreeClassifier(labelCol=\"target\")\n",
    "# Multilayer Perceptron model\n",
    "mp = MultilayerPerceptronClassifier(labelCol=\"target\", layers=[4, 5, 4, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a071ad15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Train NaiveBayes model\n",
    "nb_model = nb.fit(train_classification_df)\n",
    "# Train Random Forest model\n",
    "rf_model = rf.fit(train_classification_df)\n",
    "# Train Decision Tree model\n",
    "dc_model = dc.fit(train_classification_df)\n",
    "# Train Multilayer Perceptron model\n",
    "mp_model = mp.fit(train_classification_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4e62eefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the results on test data:\n",
    "# Transformed test data for NaiveBayes model\n",
    "nb_test_result = nb_model.transform(test_classification_df)\n",
    "# Transformed test data for Random Forest model\n",
    "rf_test_result = rf_model.transform(test_classification_df)\n",
    "# Transformed test data for Decision Tree model\n",
    "dc_test_result = dc_model.transform(test_classification_df)\n",
    "# Transformed test data for Multilayer Perceptron model\n",
    "mp_test_result = mp_model.transform(test_classification_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "49e9bcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the evaluator\n",
    "evaluator_classification = evaluation.MulticlassClassificationEvaluator(\n",
    "    predictionCol=\"prediction\", labelCol=\"target\", metricName=\"accuracy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1aa4c3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive-Bayes Classifier accuracy:  1.0\n",
      "\n",
      "Random Forest Classifier accuracy:  0.9142857142857143\n",
      "\n",
      "Decision Tree Classifier accuracy:  0.9142857142857143\n",
      "\n",
      "Maltilayer Preceptron Classifier accuracy:  0.9428571428571428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/03 14:11:51 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 1966105 ms exceeds timeout 120000 ms\n",
      "21/10/03 14:11:51 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "# Check the model accuracy\n",
    "print(\n",
    "    \"Naive-Bayes Classifier accuracy: \",\n",
    "    evaluator_classification.evaluate(nb_test_result),\n",
    ")\n",
    "print()\n",
    "print(\n",
    "    \"Random Forest Classifier accuracy: \",\n",
    "    evaluator_classification.evaluate(rf_test_result),\n",
    ")\n",
    "print()\n",
    "print(\n",
    "    \"Decision Tree Classifier accuracy: \",\n",
    "    evaluator_classification.evaluate(dc_test_result),\n",
    ")\n",
    "print()\n",
    "print(\n",
    "    \"Maltilayer Preceptron Classifier accuracy: \",\n",
    "    evaluator_classification.evaluate(mp_test_result),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb3083d",
   "metadata": {},
   "source": [
    "# 3. Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "978e9a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+-----------------+----------------+------+\n",
      "|sepal length (cm)|sepal width (cm)|petal length (cm)|petal width (cm)|target|\n",
      "+-----------------+----------------+-----------------+----------------+------+\n",
      "|              5.1|             3.5|              1.4|             0.2|     0|\n",
      "|              4.9|             3.0|              1.4|             0.2|     0|\n",
      "+-----------------+----------------+-----------------+----------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Form dataset\n",
    "clustering_df = skdataset_to_sparkdataset(load_iris)\n",
    "# Remove Null values & Duplicates\n",
    "rem_null_duplicates(clustering_df)\n",
    "# Let us visualize dataset\n",
    "clustering_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "310b20bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|            features|target|\n",
      "+--------------------+------+\n",
      "|[6.15892840883878...|     0|\n",
      "|[5.9174018045706,...|     0|\n",
      "+--------------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We use VectorAssembler to form features-column consisting of array of features out of the available features\n",
    "assembler_clustering = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"sepal length (cm)\",\n",
    "        \"sepal width (cm)\",\n",
    "        \"petal length (cm)\",\n",
    "        \"petal width (cm)\",\n",
    "    ],\n",
    "    outputCol=\"raw_features\",\n",
    ")\n",
    "\n",
    "clustering_df = assembler_clustering.transform(clustering_df)\n",
    "\n",
    "# We will scale the features column\n",
    "clustering_df = scaler.fit(clustering_df).transform(clustering_df)\n",
    "\n",
    "# Since we only want 2 columns \"features\" &\"target\" to train the model, we will not select other columns\n",
    "clustering_df = clustering_df.select(\"features\", \"target\")\n",
    "\n",
    "# Visualize the cleaned dataset\n",
    "clustering_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f718f044",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# To get the proper number of clusters, we use the elbow method\n",
    "\n",
    "# Let us count the model-costs for clusters ranging from 2 to 6\n",
    "model_cost_list = []\n",
    "for i in range(2, 7):\n",
    "    km = KMeans(k=i)\n",
    "    km_model = km.fit(clustering_df)\n",
    "    model_cost_list.append(km_model.summary.trainingCost)\n",
    "\n",
    "# To get the elbow point, we will plot graph of model-costs against the cluster numbers\n",
    "plt.plot(list(range(2, 7)), model_cost_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a79776",
   "metadata": {},
   "source": [
    "From above graph, it appears that elbow point is at \"3\". So we consider 3 optimal clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a5b6c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/14 18:42:36 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "21/10/14 18:42:36 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+----------+\n",
      "|            features|target|prediction|\n",
      "+--------------------+------+----------+\n",
      "|[6.15892840883878...|     0|         1|\n",
      "|[5.9174018045706,...|     0|         1|\n",
      "+--------------------+------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Form KMeans model with number of clusters (\"k\") as 3\n",
    "km = KMeans(k=3)\n",
    "\n",
    "# Train KMeans model\n",
    "km_model = km.fit(clustering_df)\n",
    "\n",
    "# Transform the dataset so we can get the cluster predictions\n",
    "clustering_result_df = km_model.transform(clustering_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6ba9517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster distribution as per MODEL PREDICTION: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         0|   53|\n",
      "|         1|   50|\n",
      "|         2|   47|\n",
      "+----------+-----+\n",
      "\n",
      "None\n",
      "\n",
      "---------------------------------------------------\n",
      "\n",
      "Cluster distribution as per MODEL PREDICTION: \n",
      "+------+-----+\n",
      "|target|count|\n",
      "+------+-----+\n",
      "|     0|   50|\n",
      "|     1|   50|\n",
      "|     2|   50|\n",
      "+------+-----+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Let us check the number of points in 3 clusters as per MODEL PREDICTION\n",
    "print(\"Cluster distribution as per MODEL PREDICTION: \")\n",
    "print(clustering_result_df.groupBy(\"prediction\").count().orderBy(\"prediction\").show())\n",
    "print()\n",
    "print(\"---------------------------------------------------\")\n",
    "print()\n",
    "# Let us check the ACTUAL number of points in 3 clusters\n",
    "print(\"Cluster distribution as per MODEL PREDICTION: \")\n",
    "print(clustering_df.groupBy(\"target\").count().orderBy(\"target\").show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2944eef1",
   "metadata": {},
   "source": [
    "From above, we see that KMeans has clustered the observations in 3 clusters. The count appears to be very close to the actual ones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e240e280",
   "metadata": {},
   "source": [
    "# 4. Recommendation System\n",
    "\n",
    "Currently in Pyspark, only Collaborative Filtering is supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "88d41983",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 869:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+\n",
      "|book_id|user_id|rating|\n",
      "+-------+-------+------+\n",
      "|      1|    314|     5|\n",
      "|      1|    439|     3|\n",
      "+-------+-------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Dataset Location: https://www.kaggle.com/zygmunt/goodbooks-10k/version/5\n",
    "\n",
    "# Form dataset\n",
    "recommendation_df = spark.read.csv(\n",
    "    \"Spark_for_Machine_Learning/Recommender_Systems/ratings.csv\",\n",
    "    inferSchema=True,\n",
    "    header=True,\n",
    ")\n",
    "\n",
    "# Remove Null values & Duplicates\n",
    "rem_null_duplicates(recommendation_df)\n",
    "\n",
    "# Let us visualize dataset\n",
    "recommendation_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d6137fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "981756"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Length of recommendation_df is: \", recommendation_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7e1056d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train and test\n",
    "train_recommendation_df, test_recommendation_df = recommendation_df.randomSplit(\n",
    "    [0.8, 0.2]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5758e288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form the ALS (Alternating Least Squares) model\n",
    "als = ALS(\n",
    "    userCol=\"user_id\",\n",
    "    itemCol=\"book_id\",\n",
    "    ratingCol=\"rating\",\n",
    ")\n",
    "\n",
    "# Train the ALS (Alternating Least Squares) model\n",
    "als_model = als.fit(train_recommendation_df)\n",
    "\n",
    "# Transform the dataset using the model for evaluating the model\n",
    "result_train_df = als_model.transform(train_recommendation_df)\n",
    "\n",
    "# Let us evaluate the model\n",
    "regression_evaluator = RegressionEvaluator(\n",
    "    predictionCol=\"prediction\",\n",
    "    labelCol=\"rating\",\n",
    ")\n",
    "recommendation_evaluator = regression_evaluator.evaluate(result_train_df)\n",
    "print(\n",
    "    \"RSME for the Recommendation System model on train cases is \",\n",
    "    recommendation_evaluator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0f67e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us visualize the transformed dataset\n",
    "test_recommendation_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f9298752",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 956:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+\n",
      "|book_id|user_id|rating|\n",
      "+-------+-------+------+\n",
      "|      1|   1169|     4|\n",
      "|      1|  10335|     4|\n",
      "+-------+-------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Let us recommend products for 2 users as sample from test_recommendation_df\n",
    "# Form list of 2 random users\n",
    "recommendation_test_users = set()\n",
    "i = 0\n",
    "while len(recommendation_test_users) < 2:  # because we are testing on 2 sample users\n",
    "    recommendation_test_users.add(\n",
    "        test_recommendation_df.select(\"user_id\").collect()[i][0]\n",
    "    )\n",
    "    i += 1\n",
    "recommendation_test_users = list(recommendation_test_users)\n",
    "print(\"The randomly selected 2 users are\")\n",
    "print()\n",
    "\n",
    "# Let us recommend books for these 2 users\n",
    "for user in recommendation_test_users:\n",
    "    sample_case = test_recommendation_df[test_recommendation_df[\"user_id\"] == user]\n",
    "    result_sample_cases = als_model.transform(sample_case)\n",
    "    print(f\"Recommendation for user {user} is: \")\n",
    "    result_sample_cases.show()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02545a6",
   "metadata": {},
   "source": [
    "# 5. Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ef8061",
   "metadata": {},
   "source": [
    "#### For Natural Language Processing, we will be using dataset of SMS texts which are classified into spam or ham.\n",
    "We will train the model to classify the text. <br> \n",
    "#### Process:\n",
    "1. Tokenize the SMS in words - use pyspark.ml.feature.RegexTokenizer()\n",
    "2. Remove Stop-Words (words that do not add value to the sentence) - use pyspark.ml.feature.StopWordsRemover()\n",
    "3. Use TF-IDF (term frequency–inverse document frequency) to convert words into vectors. These vectors will be the features for our model training - use pyspark.ml.feature.HashingTF & pyspark.ml.feature.IDF\n",
    "4. Convert the word-label (spam/ham) into vectors (0/1) i.e. spam as 0 & ham as 1 - use pyspark.ml.feature.StringIndexer()\n",
    "5. Train the model with features and labels - use Naive-Bayes Classifier\n",
    "6. Evaluate the model\n",
    "7. Predict sample cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c47e278",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "nlp_data = spark.read.csv(\"SMSSpamCollection\", inferSchema=True, sep=\"\\t\")\n",
    "\n",
    "# Let us check how the dataset looks\n",
    "nlp_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a830dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us rename the columns for convenience\n",
    "nlp_data = nlp_data.withColumnRenamed(\"_c0\", \"text_label\")\n",
    "nlp_data = nlp_data.withColumnRenamed(\"_c1\", \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f99d7ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us build a pipeline which tokenizes the text and converts it into word-vector features\n",
    "regex_state = RegexTokenizer(inputCol=\"text\", outputCol=\"tokenized_text\")\n",
    "stopword_removed_state = StopWordsRemover(\n",
    "    inputCol=\"tokenized_text\", outputCol=\"text_wo_stopwords\"\n",
    ")\n",
    "hashTF_state = HashingTF(inputCol=\"text_wo_stopwords\", outputCol=\"hashed_tf\")\n",
    "idf_state = IDF(inputCol=\"hashed_tf\", outputCol=\"features\")\n",
    "\n",
    "pipeline_route = Pipeline(\n",
    "    stages=[\n",
    "        regex_state,\n",
    "        stopword_removed_state,\n",
    "        hashTF_state,\n",
    "        idf_state,\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Let us convert spam/ham to vectors 0/1\n",
    "string_labelling = StringIndexer(inputCol=\"text_label\", outputCol=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2889834",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Using pipeline to transform the dataframe\n",
    "nlp_data = pipeline_route.fit(nlp_data).transform(nlp_data)\n",
    "nlp_data = string_labelling.fit(nlp_data).transform(nlp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2c869f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need only 2 columns \"features\" and \"label\" to train our model. So we can ignore the remaining columns.\n",
    "nlp_data = nlp_data.select([\"features\", \"label\"])\n",
    "\n",
    "# Split dataset into train and test\n",
    "train_nlp_data, test_nlp_data = nlp_data.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a728c3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate Naive-Bayes classifier model\n",
    "nbc = NaiveBayes(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# Train the model using training dataset\n",
    "nbc_model = nbc.fit(train_nlp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dea9d81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test performance of the model on test dataset\n",
    "test_nlp_data_result = nbc_model.transform(test_nlp_data)\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    predictionCol=\"prediction\", labelCol=\"label\", metricName=\"accuracy\"\n",
    ")\n",
    "test_eval_score = evaluator.evaluate(test_nlp_data_result)\n",
    "print(\"Test accuracy: \", test_eval_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6052106e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us check the model on new SMS.\n",
    "def show_category(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns if the entered text is Spam or Ham\n",
    "    Args:\n",
    "        text(str) - SMS text\n",
    "    Returns:\n",
    "        str : Whether the text is Spam or Ham\n",
    "    \"\"\"\n",
    "    sample_df_2 = spark.createDataFrame([{\"text\": sample_text}])\n",
    "    sample_df_2 = pipeline_route.fit(sample_df_2).transform(sample_df_2)\n",
    "    predicted_score = (\n",
    "        nbc_model.transform(sample_df_2)\n",
    "        .select([\"text\", \"prediction\"])\n",
    "        .collect()[0][\"prediction\"]\n",
    "    )\n",
    "    if predicted_score == 0:\n",
    "        return \"spam\"\n",
    "    else:\n",
    "        return \"ham\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24e833d",
   "metadata": {},
   "source": [
    "#### Demo cases:\n",
    "For demonstration, we will pass random text samples from our dataframe without the label and see the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d2cd21e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spam\n",
    "spam_text_1 = \"England v Macedonia - dont miss the goals/team news. Txt ur national team to 87077 eg ENGLAND to 87077 Try:WALES, SCOTLAND 4txt/ú1.20 POBOXox36504W45WQ 16+\"\n",
    "spam_text_2 = \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\"\n",
    "# Ham\n",
    "ham_text_1 = \"I HAVE A DATE ON SUNDAY WITH WILL!!\"\n",
    "ham_text_2 = \"Are you this much buzy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e10bdef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/14 07:49:18 WARN DAGScheduler: Broadcasting large task binary with size 8.0 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction for spam_text_1 is  spam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/14 07:49:20 WARN DAGScheduler: Broadcasting large task binary with size 8.0 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction for spam_text_2 is  spam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/14 07:49:21 WARN DAGScheduler: Broadcasting large task binary with size 8.0 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction for ham_text_1 is  spam\n",
      "Model prediction for ham_text_2 is  spam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/14 07:49:22 WARN DAGScheduler: Broadcasting large task binary with size 8.0 MiB\n"
     ]
    }
   ],
   "source": [
    "print(\"Model prediction for spam_text_1 is \", show_category(spam_text_1))\n",
    "print(\"Model prediction for spam_text_2 is \", show_category(spam_text_2))\n",
    "print(\"Model prediction for ham_text_1 is \", show_category(ham_text_1))\n",
    "print(\"Model prediction for ham_text_2 is \", show_category(ham_text_2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
